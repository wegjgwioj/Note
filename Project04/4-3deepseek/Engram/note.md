 To Do:
 
 DeepSeek Engram è¿™ç¯‡è®ºæ–‡ï¼Œä»¥åŠå®ƒèƒŒåçš„ä¸¤æ¡æŠ€æœ¯æ”¯çº¿ï¼šMemory Network å’Œ N-gramã€‚ç†è§£äº†è¿™äº›èƒŒæ™¯å·¥ä½œï¼Œå†çœ‹ Engram å°±ä¼šå‘ç°æ°´åˆ°æ¸ æˆã€‚

ğŸ“š æœ¬æœŸä¸²è®²çš„è®ºæ–‡/èƒŒæ™¯:

ã€Memory æ”¯çº¿ã€‘
â€¢ Facebook 2019 - Language Models as Knowledge Bases
â€¢ Google 2020 - T5 ä½œä¸º Knowledge Base
â€¢ 2021 - Transformer FFN å±‚æ˜¯ Key-Value Memory
â€¢ Microsoft 2022 - Knowledge Neurons
â€¢ Facebook 2019 - Product Key Memory (PKM)
â€¢ Meta - Memory Layer æ‰©å±•åˆ° 128B å‚æ•°
â€¢ DeepMind 2022 - Retrieval-Enhanced Transformer (RETRO)
â€¢ Google 2023 - External Memory æå‡æ¨¡å‹èƒ½åŠ›

ã€N-gram æ”¯çº¿ã€‘
â€¢ ä¼ ç»Ÿ N-gram è¯­è¨€æ¨¡å‹åŸºç¡€
â€¢ Google 2022 - N-Grammerï¼šç”¨ N-gram Embedding å¢å¼º Transformer
â€¢ Google 2025 - Scaling Embedding Layer

ã€DeepSeek Engramã€‘
â€¢ æ ¸å¿ƒæ€æƒ³ï¼šç”¨ N-gram Embedding + Hash æŸ¥è¡¨å®ç°é«˜æ•ˆçŸ¥è¯†æ£€ç´¢
â€¢ æŠ€æœ¯è¦ç‚¹ï¼šSparsityã€Gating æœºåˆ¶ã€Memory Hierarchy
â€¢ ä¸ MoE çš„å…³ç³»ï¼šEngram æ˜¯ MoE çš„è¡¥å……

ğŸ¯ æ ¸å¿ƒè§‚ç‚¹:
1. Transformer çš„ FFN å±‚æœ¬è´¨æ˜¯ä¸€ä¸ª Key-Value Memory
2. Sparsity æ˜¯æ‰“ç ´ã€Œä¸å¯èƒ½ä¸‰è§’ã€çš„å…³é”®ï¼ˆPerformance / Compute / Model Sizeï¼‰
3. é€šè¿‡ Hash æŸ¥è¡¨å®ç° O(1) å¤æ‚åº¦çš„ N-gram æ£€ç´¢
4. Engram è®©æ¨¡å‹ä¸ç”¨ã€Œè®¡ç®—ã€å°±èƒ½ã€Œè®°ä½ã€å¸¸è¯†çŸ¥è¯†
5. DeepSeek çš„å·¥ä½œæ˜¯å‰äººç ”ç©¶çš„é›†å¤§æˆè€…

