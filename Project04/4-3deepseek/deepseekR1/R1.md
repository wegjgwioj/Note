# 一、Background
## 1 how far AGI is from us?
AGI (Artificial General Intelligence) refers to highly autonomous systems that outperform humans at most economically valuable work. Current AI systems are typically narrow in scope, excelling at specific tasks but lacking the generality and adaptability of human intelligence.
### 1.1  supervised learning ?
    task1 & task2  not flexible enough
#### 1.1.1 why not flexible enough?
        - need large labeled dataset
        - can only handle specific tasks
        - poor generalization to new tasks
### 1.2  reinforcement learning ?
    task3 & task4  sample inefficient
#### 1.2.1 why sample inefficient?
        - need many interactions with environment
        - slow learning process
        - hard to transfer knowledge between tasks

### 1.3 some excellent research
#### 1.3.1 DeepMind 
 RL   -->  game /Science

#### 1.3.2 OpenAI 
   RL  -->  Robotics /games
   |
   -->  GPT
####  1.3.3  Yang LeCun
    JEPA --> world model

## 2 RL(Reinforcement Learning)  
### 2.1 Supervised Learning 
  input  -->  model  -->  output--> loss  --> backpropagation 

  * out of distribution
### 2.2 Reinforcement Learning 
  state  -->  model  -->  action  --> environment --> reward + next state
#### reward 
  1 Sparse --> train/optimize hard
  2 Define reward function hard  
    { 2.1 verifiable domain  2.2 unverifiable domain : human reference }  
    **reward hacking**  
#### dynamics intend with environment
  dynamics: how the environment changes over time
#### exploration vs exploitation
  exploration: trying new actions to discover their effects
  exploitation: choosing actions that are known to yield high rewards

#### policy based & value based

#### AlphoGo
   Search(+RL)
   policy net + value net + MCTS
>Stage 1: supervised learning from human expert games to train the policy network.
>Stage 2: reinforcement learning by playing games against itself to improve the policy network.
>Stage 3: combining the policy and value networks with Monte Carlo Tree Search (MCTS) to select moves during gameplay.

#### AlphoZero
   unuse  surpervised learning
   self-play + MCTS

## 3 GPT(Generative Pre-trained Transformer)
### 3.1 Why transformer is so powerful to LLM?
   NLP :  rule  --> statistics model --> RNN --> Transformer

   Vision :  SVM+...  --> CNN --> Vision Transformer

   bias-variance tradeoff 

### 3.2 Scale up
    more data + bigger model + more compute
    
###  GPT
 GPT-1  -->  GPT-2  --> ...
 1st :pre-train <--data quality
  2nd :post-train 
   a. SFT <--high-quality cold start data
   b. human annotation<--RLHF
      i. reward model
      ii. **DPO**
   c. reward model --> **PPO**

Now : SFT ->DPO -> PPO

# 二、DeepSeek

## DeepSeek-R1 :How to make large language models learn to think for themselves?

## What COT (Chain of Thought) is?
  COT is a technique used in LLMs to improve their reasoning abilities by breaking down complex problems into smaller, manageable steps. This approach allows the model to generate intermediate reasoning steps before arriving at a final answer, enhancing its ability to tackle multi-step problems.

###  Zero-COT 
  The model is prompted to generate a chain of thought without any prior examples. This approach relies on the model's inherent reasoning capabilities to produce intermediate steps leading to the final answer.

### Big-Bench & Big-Bench Hard
  Big-Bench is a benchmark dataset designed to evaluate the performance of large language models across a wide range of tasks, including reasoning, comprehension, and problem-solving. Big-Bench Hard is a subset of this dataset that focuses on particularly challenging tasks that require advanced reasoning skills.

### Some methods to enhance COT:
1. **Self-Consistency**: This method involves generating multiple reasoning paths for a given problem and then aggregating the results to arrive at a final answer. By considering various possible chains of thought, the model can improve its accuracy and robustness in reasoning tasks.
2. **Least-to-Most Prompting**: This technique breaks down complex problems into a series of simpler sub-problems. The model is prompted to solve each sub-problem sequentially, building up to the final solution. This step-by-step approach helps the model manage complexity and enhances its reasoning capabilities.

3.**PRM & ORM**: These are additional techniques that can be employed to further refine the model's reasoning process. PRM (Prompt Refinement Method) focuses on optimizing the prompts used to elicit better responses from the model, while ORM (Output Refinement Method) aims to improve the quality of the generated outputs through post-processing techniques.

 **R1 use SFT + Final-answer RL + Majority Voting**

### Results
  The experiments conducted using the DeepSeek-R1 approach demonstrated significant improvements in the reasoning abilities of large language models. By employing techniques such as Self-Consistency and Least-to-Most Prompting, the models were able to generate more accurate and coherent chains of thought, leading to better performance on complex reasoning tasks.


  # 三、R1's Dark Horse Journey: How Did It Surpass O1? Minor Tweaks or a Complete Overhaul?

  ## Reward Model Differences
  R1 used a rule-based reward model, while O1 employed a learned reward model. The rule-based approach in R1 provided more consistent and interpretable feedback, which may have contributed to its superior performance.

 ## Disadvantages of MCTS
Essentially, it is a search algorithm. Its advantage lies in finding approximate optimal solutions with limited computational resources, but its drawbacks are also obvious: while the possibilities in Go are finite, for open-domain tasks like natural language generation, the search space is enormous, making it difficult for MCTS to effectively explore all possible options, which in turn leads to the poor quality of the generated text.



## High-quality SFT data activates the model's reasoning capabilities
R1 uses high-quality SFT data to activate the model's reasoning capabilities, while O1 relies on more general datasets. High-quality data helps the model better understand complex tasks, thereby improving its overall performance.

## Overthinking & Underthinking   issue


# 四、GRPO
GRPO (Generative Reinforcement Policy Optimization) is an advanced reinforcement learning algorithm designed to optimize the performance of generative models, particularly in the context of large language models (LLMs). GRPO combines the strengths of policy optimization techniques with the generative capabilities of LLMs to enhance their ability to generate coherent and contextually relevant text.

## GRPO训练Gemma3:
https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb

## DeepSeekMath:
https://arxiv.org/pdf/2401.02954

## TRPO:
https://arxiv.org/pdf/1502.05477

## PPO:
https://arxiv.org/pdf/1707.063473

 ## PoT:
https://arxiv.org/pdf/2211.12588

## ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving:
https://arxiv.org/abs/2309.17452



"我认为GRPO并没有优化PPO，PPO这个算法本身没有什么问题。只能说GRPO更适合结果导向的场景。GRPO除了更节省资源，我觉得还有一个关键点，PPO的价值网络是对token级别的评估，这个对于长思维链推理是很难做到的，因为思考过程经常会有反思，那反思之前的内容是错的，那价值网络怎么去评估反思之前的状态累计价值呢？但是反思这个过程本身又不能说是没有价值的，这种动态修正的特性使得准确的过程价值评估变得极其困难，既然很难还不如不要这个价值网络。如果在需要考虑细粒度的过程奖励的场景，我认为PPO仍然是更好的选择。"


