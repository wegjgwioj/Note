# 一、Temperature 

 中国的首都是（） （）

粗略理解：

可能结果 向量集$Z=[0.9,0.8,...]   \to Z^`  =  [softmax(\frac{e^{z_1}}{e^{z_1}+e^{z_2}+···+e^{z_n}}),····]$     ---- （1-1）

北 0.8 南0.1 东0.01... $\to$   输出 北

## 那 Temperature  是干嘛的呢？

Temperature 记为  $T$

则（1-1）变为

$Z=[0.9,0.8,...]   \to Z^`  =  [softmax(\frac{e^{\frac  {z_1}{T}}}{e^{\frac  {z_1}{T}}+e^{\frac  {z_2}{T}}+···+e^{\frac  {z_n}{T}}}),····]$   ---（1-2）

那其实 （1-1）可以理解为T=1的形式

可以试着计算一下低维 可以得出结论：

**Temperature 越低答案越唯一越偏向概率最大的   ；Temperature越高概率差距被降低 答案多元 =胡说八道**

##  实践意义

垂直领域的选择

测试的时候 T=0 保证的复现能力

T 取较大值生成多个 后 投票 取最终答案

<end>的产生影响也大进而控制 输出保护文本长短

  安全矫正

对照：设置 T=0 和 T=1 如果 两个结果极其相似 那么 说明llm对自己的结果置信度比较大

# 二、投机采样

它的核心逻辑是：**用一个“跑得快”的小模型去预言，用一个“更有权”的大模型去审稿。**

## 为什么需要投机采样？

在大模型推理中，最大的瓶颈不是计算量，而是 **GPU 显存带宽（Memory Bound）**。

- 每生成一个 Token，大模型（Target Model）都要把几百 GB 的参数从显存搬进计算核心看一遍。
- 无论你生成的是“的”还是“广义相对论”，搬运参数的时间是一样长的。这太浪费了！

##  投机采样的具体步骤

投机采样将推理过程变成了 **“草拟 - 验证”** 的循环：

### 1. 快速草拟 (Drafting)

由一个轻量级的小模型（Draft Model，比如 1B 或 1.8B 的模型）连续预测接下来的 $K$ 个 Token。

- 因为小模型参数少、推理极快，它能迅速给出一串“草稿”。
- 例如，小模型预测接下来的词是：`“人工智能”`、`“是”`、`“未来”`。

### 2. 并行验证 (Verification)

大模型（Target Model）不再一个字一个字地吐，而是**一次性**读入小模型给出的这 $K$ 个词，进行并行计算。

- 大模型会计算出在已知前文的情况下，这 $K$ 个词出现的概率。
- 关键点：大模型处理 1 个 Token 和并行验证 5 个 Token 的耗时几乎是相近的。

### 3. 接受或拒绝 (Acceptance)

大模型根据预设的策略（如拒绝采样）决定保留哪些词：

- 如果大模型觉得小模型写得对（概率匹配），就**直接收录**。
- 如果在第 3 个词发现了错误，大模型会**拒绝**掉第 3 个词及其后续所有词，并顺便给出一个**正确**的预测。

##  为什么它能加速？

- **最坏情况：** 小模型一个都没猜对。大模型验证完后全部拒绝，但也顺便产出了一个正确 Token。此时性能略低于直接用大模型推理（多了小模型的一点开销）。
- **理想情况：** 小模型连续猜对 5 个词。大模型一次验证就能产出 5-6 个 Token，**推理速度直接提升 2-3 倍**。

## 实践意义

1. **小模型如何选择？**

   小模型必须和大模型共用一套词表（Tokenizer），否则无法对齐；且小模型的生成风格要尽量贴合大模型。

2. **除了投机采样，还有哪些无损加速方案？**

   答：**Medusa (美杜莎)**。它不需要额外的小模型，而是在大模型顶部加几个“草稿头（Heads）”来同时预测后续多个位置的 Token。

3. **在什么场景下加速效果最好？**

   答：在 **Input Bound（输入密集型）** 或者 **低并发** 场景下效果最明显。如果 GPU 已经满载了，投机采样的额外开销反而可能拖累性能。

# 三、 KV cache

这里deepseek 那个视频讲的很好

# 四 、top-k 和 top-p
在生成式 AI（如 GPT 或 Gemini）的推理过程中，**Top-k** 和 **Top-p (Nucleus Sampling)** 是两种最常用的采样策略。它们的作用都是为了平衡生成内容的**多样性**和**准确性**，防止模型产生重复、枯燥或完全不合逻辑的词汇。

---

## 1. 核心背景：概率分布

在模型预测下一个词时，它会给词表中的每个词分配一个概率。比如，输入“我爱吃”，模型可能会给出：

* **苹果**: 0.4
* **香蕉**: 0.3
* **米饭**: 0.1
* **手机**: 0.001 (长尾词)

如果不加限制（即贪婪搜索），模型永远只选概率最大的词，生成内容会非常死板。Top-k 和 Top-p 则是通过不同的规则来“切掉”概率较低的词。

---

## 2. Top-k 采样（基于数量）

**Top-k** 的逻辑非常简单：在解码时，只从概率最高的前  个词中进行采样。

* **工作原理**：将所有词按概率从高到低排序，保留前  个，将剩下的词概率置为 0，然后重新归一化剩下的词。
* **特点**：
* **固定窗口**：无论概率分布如何，候选池的大小永远是 。
* **风险**：如果  太小，生成内容缺乏创意；如果  太大，可能会选到概率极低、完全不通顺的词。


* **例子**：如果 ，无论第 4 名词的概率是 10% 还是 0.1%，它都会被剔除。

---

## 3. Top-p 采样（基于质量/累积概率）

**Top-p**（也叫核采样，Nucleus Sampling）是目前更主流的方法。

* **工作原理**：不是取前  个词，而是取累计概率达到阈值  的最小词集。
* **特点**：
* **动态窗口**：候选池的大小是根据概率分布**动态调整**的。
* **逻辑**：如果模型对下一个词非常确定（概率分布集中），候选池就很小；如果模型比较迷茫（概率分布平缓），候选池就会变大。


* **例子**：如果 ：
* 情景 A：前两个词的概率加起来就到了 0.92，那么候选池只有这两个词。
* 情景 B：前 100 个词的概率加起来才到 0.9，那么候选池就会包含这 100 个词。



---

## 4. 两者的对比与应用

| 特性 | Top-k | Top-p (Nucleus) |
| --- | --- | --- |
| **筛选依据** | 词的数量 | 概率的累积和 |
| **灵活性** | 较低（固定数量） | 较高（动态调整） |
| **主要效果** | 过滤掉低概率的长尾词 | 确保候选词集包含足够的“信息量” |
| **推荐设置** | 通常设为 40-50 | 通常设为 0.9 或 0.95 |

### 实际建议：

1. **联合使用**：大多数推理框架支持同时开启。模型会先进行 Top-k 筛选，再在剩下的词里进行 Top-p 过滤。
2. **搭配 Temperature（温度）**：
* 如果你希望结果更**稳定、客观**（如写代码、回答事实）：调低 Temperature，调低 Top-p。
* 如果你希望结果更**有创意、多样**（如写小说、头脑风暴）：调高 Temperature，调高 Top-p 或 Top-k。


# 五、Temperature=0 为什么相同的输入有时会输出不同结果？

理论上，`Temperature=0` 意味着模型每次都选择概率最大的那个词（Greedy Search），这**应该**是确定的。但在实际工程落地中，由于**硬件计算特性**、**模型架构**和**分布式系统**的复杂性，确实会出现“相同的输入，输出却微小不同”的现象。

以下是导致这种现象的几个核心原因，按影响程度排序：

---

### 1. GPU 的浮点运算不确定性（最底层原因）

这是最本质的物理/数学原因。现代大模型都在 GPU 集群上运行，而 GPU 为了追求极致的并行计算速度，通常会牺牲掉微小的精度确定性。

* **加法结合律失效**：在浮点数运算中， 和  的结果在极高精度下是不一样的（浮点数舍入误差）。
* **并发累加（Atomic Add）**：当 GPU 的成千上万个核心同时计算矩阵乘法并累加结果时，**执行顺序是随机的**。
* **蝴蝶效应**：这种微小的误差（可能只有  级别）在深层神经网络中层层传递，可能会在某一层导致 Logits（原始分数）发生极其微小的变化。
* 如果此时候选词 A 和 候选词 B 的概率极度接近（比如 A 是 40.000001%，B 是 40.000000%），这点微小的硬件噪音就可能导致排序反转，从而选出不同的词。一旦选错一个词，后续生成的路径就完全改变了。

### 2. Mixture of Experts (MoE) 架构的影响

* **专家路由的负载均衡**：MoE 模型在推理时，会动态选择将 token 发送给哪些“专家”（Experts）。为了防止某个专家过载（计算排队），路由算法通常会包含**负载均衡机制**或微小的随机扰动。
* **丢弃机制**：在极高并发下，如果被选中的专家缓冲区满了，部分 token 可能会被丢弃或被强制路由到次优专家（虽然后期模型对此有优化，但逻辑上仍存风险）。这意味着即使 Prompt 一样，系统负载不同，激活的专家路径可能略有差异。

### 3. "Temperature=0" 的实现方式差异

在很多推理框架或 API 中，`Temperature=0` 并不一定是真的数学上的 0。

* **数值稳定性**：除以 0 会导致错误。因此，很多代码实现实际上是将 Temperature 设为一个**极小值**，然后进行采样。只要不是纯粹的 `argmax`（取最大值），就仍然保留了极其微小的随机采样的可能性。
* **Top-k 的截断**：如果 Top-k 截断发生在排序不稳定的时候（例如两个词分数完全一样），不同的排序算法（QuickSort vs MergeSort）可能会导致保留下来的词表不同。

### 4. 分布式推理与并行策略

大模型通常因为太大，无法装在一张显卡里，需要使用 **Tensor Parallelism (TP)** 或 **Pipeline Parallelism (PP)**。

* **通信噪音**：跨卡通信（All-Reduce）时，不同 GPU 的归约操作顺序可能不同，再次引入了浮点数精度的不确定性。

### 5. 批处理 (Batching) 与 Padding

为了提高吞吐量，推理服务通常会将多个用户的请求打包成一个 Batch 一起跑。

* 你的 Prompt 是单独跑，还是和别人的 Prompt 拼在一起跑，会影响 Padding（填充符）的位置。
* 虽然 Attention Mask 应该屏蔽掉 Padding 的影响，但在某些 Flash Attention 的优化实现中，不同的 Batch 组合可能会引入极微小的计算差异。


### 总结

当 `Temperature=0` 时，如果你的模型对下一个词的预测非常**自信**（例如概率是 99% vs 1%），那么上述的噪音通常无法撼动结果，输出是稳定的。

但如果模型处于**犹豫状态**（两个词的概率极其接近，如 30.1% vs 30.05%），硬件级别的浮点噪音就会成为“压死骆驼的最后一根稻草”，导致走了不同的分支。

**如果你需要严格的确定性（Reproducibility）：**
通常需要在调用时指定 `Seed`（随机种子），并且依靠推理引擎开启 `Deterministic Mode`（但这通常会显著降低推理速度，因为它禁止了许多 GPU 并行优化）。