# 一、What  is Fine-tuning?     

Fine-tuning (微调) 产出的是“听话的专家/助手”：
我们给它特定的数据（问题+标准答案），教它按照人类期望的格式说话。
目的：让模型适配特定任务（如：写SQL、医疗诊断、情感分析）或遵循特定指令。

在实际工程中，我认为它主要分为两个阶段：
 1. 注入知识或格式规范：通过 SFT 让模型学会特定领域的指令遵循（比如 Copilot 的代码生成格式）；
 2. 对齐价值观与偏好：通过 DPO 或 RLHF 让模型生成的答案更符合人类或业务逻辑的偏好（比如代码的安全性）。 另外，为了工程落地的高效性，现在更多采用 LoRA 等 PEFT 技术，以平衡训练成本和模型效果。

## 技术本质：
Fine-tuning 是在已有的预训练权重（Pre-trained Weights）基础上，使用特定领域或特定任务的数据集，进行进一步的梯度下降更新。
        $$\theta_{new} = \theta_{old} - \eta \cdot \nabla L(\theta)$$

    * 起点不同：初始化参数不是随机的，而是已经在万亿 token 上训练好的高质量参数。

    * 数据量级不同：预训练用 TB 级数据；微调通常用 MB 到 GB 级数据（几千到几十万条）。

    * 目标函数：通常依然是 Cross Entropy Loss（交叉熵损失），计算预测 token 与真实 token 的差异。但在 RLHF 阶段（后训练），目标函数会变成最大化奖励（Reward Maximization）。
    
---


# 二、How to Fine-tune? （以copilot为例）

微调绝不仅仅是运行一行代码，而是一个完整的**工程链路**。
####  [2.1数据准备 (Data Preparation)](#21-数据准备-data-preparation)
#### [2.2基座模型选择 ](#22-基座模型选择-base-model-selection)
#### [2.3. 配置训练参数 (Configuration)](#23-配置训练参数)
#### [2.4. 训练与监控 (Training & Monitoring)](#24-训练与监控)
#### [2.5. 评估 (Evaluation)](#25-评估)


# 2.1-数据准备 (Data Preparation)

### 第一层：数据构造 (Data Construction)

**制造数据**。

1. **Self-Instruct / 合成数据 (Synthetic Data)**：
* **痛点**：高质量的人工标注数据（如专家写的代码注释）太贵、太少。
* **做法**：利用更强的模型（如 GPT-4）生成数据来教弱模型（如 Llama-3-8B）。

    * **Evol-Instruct**：不仅是生成，还要让 GPT-4 把指令“进化”得更难（例如：增加约束条件、增加代码复杂度）。 
    * **OSS-Instruct**：针对代码场景，从 GitHub 抓取代码片段，让 GPT-4 反向生成“这段代码对应的需求是什么”，构造 `{Code} -> {Instruction}` 对。

2. **数据配比 (Data Mixture)**：
* **黄金配比**：通常是 **通用能力数据 (50%) + 垂直领域数据 (Code/SQL, 40%) + 逻辑推理/数学 (CoT, 10%)**。只喂代码会导致模型丧失日常对话能力（灾难性遗忘）。


---

### 第二层：数据清洗 (Data Cleaning) 

脏数据比没数据更可怕。对于 Copilot 这种代码生成任务，清洗尤为关键。

1. **规则清洗**：
* 去除过短的数据（< 10 tokens）。
* 去除包含 PII（个人隐私信息，如邮箱、IP）的数据——**隐私要极度敏感**。
* 去除无法解析的代码（AST Parsing Check）：如果代码连语法树都生成不了，坚决扔掉。


2. **去重 (De-duplication)**：
* **MinHash LSH**：在大规模语料中，必须用模糊去重，防止模型“死记硬背”同一段代码。
* 举个例子

**假设你爬取 GitHub 代码，为了教模型写 Python 的冒泡排序**

数据 A (原版):
```Python
def bubble_sort(arr):
    n = len(arr)
    # 遍历所有数组元素
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1] :
                arr[j], arr[j+1] = arr[j+1], arr[j]
```
数据 B (修改版 - 仅仅换了变量名和注释):

```Python
def bubble_sort(nums):  # 变量名从 arr 改成了 nums
    n = len(nums)
    # 这是一个冒泡排序算法
    for i in range(n):
        for j in range(0, n-i-1):
            if nums[j] > nums[j+1] :
                nums[j], nums[j+1] = nums[j+1], nums[j]
```
**问题**：
如果不做模糊去重：这两个文件的 MD5 完全不同，系统会把它们当成两份不同的数据喂给模型。
**后果：** 模型看了 1000 遍各种变体的冒泡排序，导致它过拟合 (Overfitting)。以后你问它“写个排序”，它大概率只能吐出冒泡排序，完全忘了快排或归并。

**MinHash LSH (Locally Sensitive Hashing)** 的作用： 它不看具体的字符，而是看特征 (N-grams/Shingles) 的重合度。如果计算出 A 和 B 的 Jaccard 相似度 > 0.8（比如逻辑结构高度相似），算法就会判定它们是“近义重复”，直接丢掉其中一份。

* **测试集泄露 (Decontamination)**：务必确保你的训练数据里，不包含 HumanEval 或 MMLU 测试集里的题。否则评测分数虚高，上线就“拉胯”。



---

### 第三层：格式标准化 (Formatting) —— 模型的接口

模型看不懂 Excel，它只能看懂特定的 JSON 结构。主流有两种格式：

**1. Alpaca 格式（单轮指令，适合工具型）**
这是最经典的格式，适合简单的 `Input -> Output` 任务。

```json
[
  {
    "instruction": "请解释下面Python代码的功能。",
    "input": "def add(a, b): return a + b",
    "output": "这是一个简单的函数，接收两个参数a和b，返回它们的和。"
  }
]

```

**2. ShareGPT 格式（多轮对话，适合 Chatbot）**

Copilot 通常需要多轮交互（比如用户让修改刚才写的代码），所以这种格式更重要。

```json
[
  {
    "conversations": [
      { "from": "human", "value": "帮我写个快排。" },
      { "from": "gpt", "value": "好的，这是Python实现的快速排序..." },
      { "from": "human", "value": "能不能改成C++版本？" },
      { "from": "gpt", "value": "没问题，这是C++版本..." }
    ]
  }
]

```

---

### 第四层：Tokenization 与 Loss Masking 

#### 1. 什么是 Loss Masking（损失掩码）？

在 SFT 微调时，我们**只计算“回答部分”的 Loss，而不计算“提问部分”的 Loss**。

* **为什么？**
* 模型不需要预测“用户问了什么”（因为这是已知的），只需要预测“我该怎么回答”。
* 如果不做 Masking，模型会浪费算力去学习用户的提问方式，导致模型变得“爱自言自语”或复读机。

具体例子 (Output)：

用户输入：今天天气怎么样？
> 没 Mask 的模型：今天天气怎么样？明天天气好吗？后天会下雨吗？ (它在模仿人类提问，而不是在回答)

> Mask 后的模型：今天晴转多云... (它只专注于生成回答)

#### 2. 实现细节（图解原理）

假设输入是：`User: hi <eos> Assistant: hello <eos>`

* **Input IDs**: `[User, :, hi, <eos>, Assistant, :, hello, <eos>]`
* **Labels**:    `[-100, -100, -100, -100, -100, -100, hello, <eos>]`
**重点**
在 PyTorch 中，`Label` 设置为 `-100` 的位置，交叉熵损失函数（CrossEntropyLoss）会自动忽略，不计算梯度。

#### 3. 填充与截断 (Padding & Truncation)

* **Padding Side**：
* **训练时**：通常用 `Right Padding`（补在后面）配合 Attention Mask 没问题。
* **推理时**：必须用 **`Left Padding`**（补在前面）。因为如果补在后面，生成的 Token 会受到 Padding Token 的干扰。

```
推理阶段 (Batch Size=1, Max Len=4)

Right Padding (错误):
[ "Hello", <pad>, <pad>, <pad> ]  -->  ??? (模型试图根据 <pad> 预测下一个词)
                         ^
                         |
                   模型关注这里

Left Padding (正确):
[ <pad>, <pad>, <pad>, "Hello" ]  -->  "World" (模型根据 "Hello" 预测下一个词)
                         ^
                         |
                   模型关注这里
```
**重点**
特别是使用 Hugging Face 的 generate() 函数时，如果没设置 padding_side='left'，会导致输出完全不连贯，这是因为 Transformer 的位置编码（Positional Embedding）在推理时需要对齐到真实的最后一个 Token。
#### 4. 特殊 Token 处理 (Special Tokens)
* **特殊 Token**：
* 对于 Copilot，需要自定义 `<tool_start>`, `<code_start>` 等 Special Tokens，并确保 Tokenizer 不会将它们拆解（split）。



# 2.2 基座模型选择 (Base Model Selection)

* **数据**是教材。
* **基座模型**就是挑选的“学生苗子”。

选错了苗子（比如选了个数学很差的模型去学写代码），教材再好也练不出来。

**选型梯队**和**决策逻辑**

---
### 1. 核心决策：选 "Base" 还是 "Instruct"？
在 HuggingFace 上，通常会看到同一个模型有两个版本，比如 `Qwen2.5-7B` 和 `Qwen2.5-7B-Instruct`。

* **Base 模型 (基座版)**：
* **特点**：只做过预训练（续写文本）。不懂“你好”，只会续写“你好吗”。
* **适用场景**：**注入大量新知识**。比如要让 Copilot 学会一种从未见过的微软内部编程语言（Internal DSL），选 Base 更好。

* **Instruct / Chat 模型 (指令对齐版)**：
* **特点**：已经做过 SFT 和 RLHF，能听懂指令。
* **适用场景**：**微调特定任务**。比如只是想让它“用特定的 JSON 格式输出代码”，选 Instruct 版本起点更高，训练收敛更快。
* **建议**：首选 Instruct 版本进行微调。** 站在巨人的肩膀上，效果更容易达标。
---
### 2. 针对 项目 做的“推荐清单”
浏览各种大模型的参数和架构后，总结得出合理的选型梯队... 

---
### 3. 关键硬件考量：显存够不够？
选模型不能光看效果，还得看显卡（GPU）能不能塞进去。
假设只有一张 **RTX 3090 (24GB 显存)** 
| 模型参数量 | 精度 (Loading) | 显存需求 (推理) | 显存需求 (微调 - QLoRA) | 可行性 |
| --- | --- | --- | --- | --- |
| **7B / 8B** | fp16 (半精度) | ~16GB | ~18-20GB | **完美** (推荐) |
| **14B** | 4-bit (量化) | ~10GB | ~16-20GB | **完美** (推荐) |
| **32B** | 4-bit (量化) | ~20GB | > 24GB (OOM) | **不可行** (除非双卡) |
| **70B** | 4-bit (量化) | ~40GB | > 48GB | **不可行** |

**建议**：
如果是个人项目或普通实验室环境，**死磕 7B 或 8B 模型**。
如果是有 A100/H100，可以直接上 **32B 或 70B**，效果会有质的飞跃。

---

### 4. 还有一个隐形坑：Context Window (上下文窗口)

**代码太长，模型截断了。**

* 普通的 Llama-2 只有 4k 长度（大概 300 行代码就满了）。
* **Qwen2.5** 和 **Llama-3.1** 支持 **128k** 甚至更长。

**注意点**：
一定要确认你选的基座模型支持 **Long Context**。

> “在选型时，特别关注了模型的 RoPE 缩放系数，确保它能处理至少 16k 的 token，因为 Copilot 场景经常需要把整个 Class 定义放入 Context 中。”

---

# 2.3 配置训练参数
### 1. 三大核心参数 (The "Big Three")
#### A. Learning Rate (学习率) —— 最关键
- **原则**：微调的学习率要比预训练**小**。
- **LoRA 黄金值**：`1e-4` 到 `2e-4`。
  - *注意：如果你做全量微调 (Full Finetune)，通常要降到 `1e-5`。但 LoRA 训练参数少，需要稍大的 LR 才能通过小窗口撬动大模型。*
- **Scheduler (调度器)**：推荐 `cosine` (余弦退火)。
- **Warmup (预热)**：设置为总步数的 `0.03` (3%)。
  - *解释*：刚开始训练时梯度可能不稳定，先用很小的 LR 热个身，防止把预训练好的权重瞬间带偏。
#### B. Epochs (轮数)

- **推荐值**：`1` 到 `3` 个 Epoch。
- **为什么这么少？**：LLM 记忆力太好了。对于指令微调（SFT），通常 **1 个 Epoch 就足够**让模型学会格式。跑太多（比如 10 Epoch）会导致模型开始背诵训练数据，出现**过拟合**（只会答题，不会泛化）。
#### C. Batch Size (批大小)
这里有个陷阱：**Micro Batch Size** vs **Global Batch Size**。

- **Micro Batch Size**：每张显卡单次计算读入的数据量（受限于显存，通常设为 1, 2, 4）。
- **Gradient Accumulation (梯度累积)**：为了模拟大 Batch Size（比如 128），我们在 Micro Batch Size = 2 的情况下，跑 64 次才更新一次权重。
- **Global Batch Size** = Micro_BS * Accumulation_Steps * GPU_Num。
- **推荐值**：总 Batch Size 建议在 `64` 到 `128` 之间。

------

### 2. LoRA 专属参数 (The PEFT Configs)

如果用 LoRA，这几个参数决定了微调的效果和显存占用。

- **Rank (r)**：LoRA 矩阵的秩。
  - **推荐值**：`8` 或 `16`。
  - *经验*：对于代码任务（Copilot），有时候 `r=64` 甚至 `128` 会更好，因为代码逻辑比自然语言复杂，需要更大的“脑容量”去适配。但起步先试 16。
- **Alpha (Scaling Factor)**：缩放系数。
  - **推荐值**：通常设置为 Rank 的 **2倍**（即 `alpha=32` if `r=16`）。
  - *作用*：类似于一个“放大器”，控制 LoRA 更新量对原模型的影响权重。
- **Target Modules (目标模块)**：
  - **推荐值**：`["q_proj", "v_proj"]` (最省显存) 或者 `["all-linear"]` (效果最好)。
  - *解释*：你可以只微调 Attention 层的 Q、V 矩阵，也可以微调所有的 Linear 层（包括 MLP 层）。**建议选 `all-linear` (或列出具体名称)，虽然慢点，但效果提升显著。**

------

### 3. 显存救命参数 (Memory Savers)

如果你的显存只有 24GB，这几个参数必须开，否则直接 OOM (Out of Memory)。

- **Gradient Checkpointing (梯度检查点)**：**必须开启 (`True`)**。
  - *原理*：用“时间换空间”。不保存中间层的激活值，反向传播时重新计算。能节省 50%-70% 的显存。
- **Quantization (量化)**：
  - `load_in_4bit=True` (QLoRA)。
  - 计算类型 `bnb_4bit_compute_dtype="bf16"` (如果有 A100/3090/4090) 或 `"fp16"` (老卡)。
- **Flash Attention 2**：
  - `use_flash_attention_2=True`。加速训练神技，大幅降低显存占用。

------

### 4. 工业级配置模板 (YAML)

假设我们要使用 **Llama-Factory** 或者自己写 **HuggingFace Trainer**，这是一份标准的配置清单。。

YAML

```yaml
### Model Configuration
model_name_or_path: "Qwen/Qwen2.5-Coder-7B-Instruct"
quantization_bit: 4          # 开启 4-bit 量化 (QLoRA)

### Method Configuration
stage: sft                   # 阶段：SFT 指令微调
do_train: true
finetuning_type: lora        # 使用 LoRA
lora_rank: 16                # r
lora_alpha: 32               # alpha
lora_target: all             # 微调所有线性层 (推荐)

### Dataset Configuration
dataset: "my_copilot_data"   # 你之前准备好的 jsonl
template: "qwen"             # 对应的 Prompt 模板 (非常重要!)
cutoff_len: 4096             # 截断长度 (Copilot 建议长一点)
overwrite_cache: true

### Training Configuration (Hyperparameters)
learning_rate: 2.0e-4        # LoRA 经典学习率
num_train_epochs: 3.0        # 跑 3 轮
per_device_train_batch_size: 2 # 单卡 Batch Size (显存小就设 1)
gradient_accumulation_steps: 16 # 梯度累积 (2 * 16 = Global Batch 32)
lr_scheduler_type: cosine
warmup_ratio: 0.03
weight_decay: 0.01           # 权重衰减 (防止过拟合)

### Efficiency & Logging
fp16: true                   # 混合精度训练 (30系/40系显卡用 bf16: true)
gradient_checkpointing: true # 【必开】省显存
logging_steps: 10            # 每 10 步输出一次日志
save_steps: 500              # 每 500 步保存一次模型 checkpoint
```

------

### 💡 “避坑指南”

在配置环节，这个 **Trick**：

**Q: "我的 Loss 降得很好，但是生成出来的回答全是乱码或者一直重复，这是参数哪里设错了？"**

- **嫌疑犯 1：EOS Token 没配对。**
  - 你在 Data Preparation 阶段加了 `<eos>`，但在 Configuration 里没告诉 Tokenizer 哪个 ID 是 EOS。导致模型讲完话了还不知道闭嘴，继续强行输出。
- **嫌疑犯 2：Prompt Template 没对齐。**
  - 你训练用了 Qwen 的模板（`<|im_start|>user...`），推理时却用了 Llama 的格式（`[INST]...`）。模型直接懵圈。**一定要确保 `template` 参数正确！
  
  **过拟合 和 格式问题不同**
  过拟合 ：得分低
  格式问题：直接就是0分
---

# 2.4 训练与监控 


### 2.4.1、仪表盘三要素 (The Dashboard)

可视化工具（如 **TensorBoard** 或 **WandB (Weights & Biases)**）。重点监控这三条线：

#### 2.4.4.1. Training Loss (训练集损失) —— “模型学进去了吗？”

- **含义**：模型在它正在学习的数据上表现如何。
- **理想形态**：像滑滑梯一样，刚开始下降很快，中间变缓，最后趋于平稳。

#### 2.4.4.2. Evaluation / Validation Loss (验证集损失) —— “模型学会举一反三了吗？”

- **含义**：模型在它**没见过**的数据上表现如何。
- **重要性**：**这才是最重要的指标！** 客户用 Copilot 时输入的都是新代码，不是你训练集里的代码。

#### 2.4.4.3. Gradient Norm (梯度范数) —— “模型学得稳不稳？”

- **含义**：当前这一步更新的力度有多大。
- **理想形态**：应该相对稳定。如果突然飙升（Spike），说明模型“岔气”了，遇到了坏数据。

------

### 2.4.2. 常见病症诊断书 (Loss Curve Diagnosis)

#### 病症 A：过拟合 (Overfitting) —— “死记硬背”

- **现象**：
  - **Training Loss**：持续下降，甚至接近 0。
  - **Eval Loss**：**先下降，然后突然开始反弹（上升）**，呈现“U型”或“对钩型”。
- **诊断**：模型开始背答案了，丧失了泛化能力。
- **Copilot 场景**：你问它一道训练集里的 LeetCode 题，它秒回；你换个变量名，它就不会了。
- **处方 (如何修)**：
  1. **早停 (Early Stopping)**：在 Eval Loss 开始反弹的那一刻停止训练。
  2. **增加数据**：数据太少，模型太强，容易过拟合。
  3. **调大 Weight Decay**：增加正则化力度。
  4. **降低 LoRA Rank**：模型脑容量太大，把它改笨一点（比如 r 从 64 降到 8）。

#### 病症 B：欠拟合 (Underfitting) —— “学不进去”

- **现象**：Training Loss 和 Eval Loss 都很高，且下降极其缓慢，或者干脆是一条直线（Flatline）。
- **诊断**：模型太笨了，或者学习率设置有问题。
- **处方**：
  1. **调大学习率 (LR)**：可能是 LR 太小（如 1e-6），模型挪不动步子。
  2. **增加 LoRA Rank**：LoRA 参数太少（如 r=1），表达能力不够，升到 16 或 32。
  3. **检查数据格式**：**最常见的原因！** 比如你的 Prompt 模板配错了，模型根本看不懂输入是啥。

#### 病症 C：Loss 震荡 (Oscillation) —— “反复横跳”

- **现象**：Loss 曲线像心电图一样剧烈上下波动，甚至不降反升。
- **诊断**：Batch Size 太小，或者学习率太大。
- **处方**：
  1. **增加 Batch Size**：使用梯度累积（Gradient Accumulation），让梯度估计更准。
  2. **减小学习率**。

#### 病症 D：Loss Spike / NaN (梯度爆炸) —— “走火入魔”

- **现象**：Loss 突然变成一个巨大的数字，然后瞬间变成 `NaN`（Not a Number）。
- **诊断**：遇到了“脏数据”或者梯度爆炸。
- **处方**：
  1. **开启 Gradient Clipping (梯度裁剪)**：限制最大梯度范数（如 `max_grad_norm = 1.0`）。
  2. **检查数据**：通常是因为某一条数据特别长，或者包含特殊的乱码字符。
  3. **换 bf16**：如果你在用 fp16 训练，可能会溢出。换成 bf16 (Bfloat16) 通常能解决。

------

###  2.4.3、不仅看 Loss，还要“实地考察”

除了看曲线，强烈建议在训练脚本里加一个 **Callback（回调函数）**。

**做什么？**

每隔 100 步（Step），让模型尝试回答同一个**固定的测试问题**，并打印出来人工看一眼。

- **Step 0**: 模型回答：“%￥#@……&” (乱码，正常)
- **Step 100**: 模型回答：“def function...” (开始有代码的样子了)
- **Step 500**: 模型回答：“def bubble_sort...” (逻辑通顺)
- **Step 1000**: 模型回答：“...（开始复读）...” (完了，过拟合了)

**这种“定性分析”往往比 Loss 曲线更能直观地告诉你模型现在的状态。**

------

### 💡 总结

监控训练就是看三个信号：

1. **Loss 降没降？** (降了才算入门)
2. **Eval Loss 反弹没？** (反弹了就是过拟合，赶紧停)
3. **生成的人话通顺吗？** (人工抽检)



# 2.5 评估 

### 评估 (Evaluation) —— 验尸报告

既然我们已经聊到了“跑分为 0”，那我们就顺理成章进入最后一步：**如何科学地评估一个 Copilot 模型？**

只看 Loss 是不能上线的，必须跑通 **Benchmarks**。

#### 1. 代码能力金标准：HumanEval & MBPP

这是Copilot 必须熟悉的两个数据集。

- **HumanEval (OpenAI)**：
  - **内容**：164 道 Python 编程题（手写算法）。
  - **格式**：提供函数签名和 Docstring，让模型补全函数体。
  - **Pass@1 指标**：让模型只写一次，能通过单元测试的概率。
  - **Pass@10 指标**：让模型写 10 个版本，只要有一个能过就算过（Copilot 常用这个，因为用户可以刷新）。
- **MBPP (Mostly Basic Python Problems)**：
  - 更基础一点的编程题，适合测试 7B 以下的小模型。

#### 2. 怎么测？(工具链)

不要自己写脚本去跑 HumanEval，因为涉及到沙箱环境（防止模型生成的代码把你的服务器删了）。

- **推荐工具**：**BigCode Evaluation Harness** 或 **Ollama** 自带的评测工具。
- **流程**：
  1. 加载微调好的模型。
  2. 对 HumanEval 的 prompt 进行推理，生成 jsonl。
  3. 在一个**隔离的 Docker 容器**里执行生成的代码，运行单元测试。
  4. 输出 Pass@k 分数。

#### 3. 常见“作弊”嫌疑 (Data Contamination)

“你的微调模型在 HumanEval 上跑了 80% 的高分，超过了 GPT-4，你觉得正常吗？”

- **回答**：不正常，极大概率是**数据泄露（Data Contamination）**。
- **原因**：你爬取的 GitHub 数据里，可能包含了别人上传的 HumanEval 解答代码。模型不是学会了写代码，而是背下了答案。
- **对策**：在数据准备阶段，必须把 HumanEval 的题干拿去和训练数据做 N-gram 查重，把泄露的数据删掉。

---

# 总结

LLM 微调 的全流程：

1. **理论**：Transformer, LoRA, Loss Masking。
2. **数据**：清洗、去重、合成数据 (Evol-Instruct)。
3. **选型**：Qwen2.5-Coder, DeepSeek-V3。
4. **配置**：LR, Rank, Gradient Checkpointing。
5. **监控**：Loss 曲线诊断。
6. **评估**：HumanEval, Pass@1, 数据泄露检测。

---
# 三、 学习常用的微调技术

目前业界主要分为 **Full Fine-tuning (全量微调)** 和 **PEFT (参数高效微调)** 两大流派。

#### 3.1. Full Fine-tuning (全量微调)

* **原理**：更新模型中**所有**参数（权重）。
* **特点**：效果通常最好，但极度消耗显存（显存需求是模型权重的 3-4 倍），且训练慢，容易遗忘通用知识。
* **适用场景**：拥有几百张 A100/H100 的大厂，或者在拥有几十亿 Token 的大规模领域数据注入时（Continual Pre-training）。

#### 3.2. PEFT (Parameter-Efficient Fine-Tuning) —— 面试重点

PEFT 的核心思想是：**冻结大部分模型参数，只训练极少量的参数（<1%）。**

以下是三种必须掌握的具体技术：

##### A. [LoRA (Low-Rank Adaptation)](##LoRA)

##### B. [QLoRA (Quantized LoRA)](#qlora)

##### C. [P-Tuning v2&Prefix Tuning](#p-tuning-v2--prefix-tuning)


---

#### 3.3总结 (Interview Cheat Sheet)

如果面试官问：“面对一个新的业务场景（比如 Copilot 的代码生成），你会选择哪种技术？”

1. **首选 LoRA/QLoRA**：
> “在资源有限或需要快速迭代的实习场景下，我会首选 **LoRA**。因为它在减少显存占用的同时，能达到接近全量微调的效果，并且方便针对不同编程语言（Python/Java）管理不同的 Adapter。”

2. **数据决定成败**：
> “但我认为，相比于纠结微调算法，**高质量的数据构建（Data Centric）** 更关键。对于 Copilot，我会花 80% 的时间清洗代码数据、构建 CoT（思维链）数据，然后用 LoRA 跑通流程。”

3. **提及 Post-training**：
> “如果 SFT 后模型能写代码但风格不符合规范，我会进一步考虑使用 **DPO (直接偏好优化)** 进行对齐，这是目前提升落地体验的关键技术。”


🗺️ 全景图：微调技术的家族谱系
首先，你要在脑海里建立这个分类体系。当面试官问“你知道哪些微调技术？”时，不要只答 LoRA，要先抛出这个分类：

* 加法型 (Additive)：在原模型里插入新的参数模块。
  代表：Adapter, LoRA (本质是旁路加法)。
* 选择型 (Selective)：不加新参数，只选一部分原参数来训练。
代表：BitFit (只训 Bias 偏置项)。

* 重构型 (Reparametrization)：利用低秩矩阵分解来更新权重。
  代表：LoRA (训练时是加法，推理时可重合)。
* 软提示型 (Soft Prompt)：在输入层动手脚。
代表：P-Tuning, Prefix-Tuning。

---


## LoRA 
### 一、 数学原理：从全量微调到低秩分解

#### 1. 背景

一个预训练好的大模型，其权重矩阵为 $W_0 \in \mathbb{R}^{d \times k}$。

在传统的全量微调（Full Fine-Tuning）中，我们需要更新所有的参数。微调后的权重可以表示为：

$$W = W_0 + \Delta W$$

其中 $\Delta W$ 是权重更新量。如果模型有 70B 参数，$\Delta W$ 也有 70B 参数，这对显存和存储都是巨大的负担。

#### 2. 核心假设：内在低秩性 (Intrinsic Rank)

LoRA 的作者基于一个假设：**模型在特定任务上的微调，其参数更新矩阵 $\Delta W$ 具有“低秩”特性。**

简单来说，虽然大模型的参数空间很大，但针对某个具体任务（比如把通用模型改成写代码的模型），真正起作用的参数变化不需要那么高的自由度，它可以被压缩。

#### 3. LoRA 的数学表达

基于上述假设，LoRA 将巨大的更新矩阵 $\Delta W$ 分解为两个极小的矩阵 $A$ 和 $B$ 的乘积：(奇异值分解)

$$W = W_0 + B A$$

- $W_0 \in \mathbb{R}^{d \times k}$：被**冻结**的预训练权重（不参与更新）。
- $B \in \mathbb{R}^{d \times r}$：降维后的上投影矩阵（训练参数）。
- $A \in \mathbb{R}^{r \times k}$：降维后的下投影矩阵（训练参数）。
- $r$：**秩 (Rank)**，这是一个超参数，通常远小于 $d$ 和 $k$（例如 $r=8, 16, 64$）。

#### 4. 前向传播计算

对于输入 $x$，加入 LoRA 后的前向计算公式变为：

$$h = W_0 x + B A x$$

这里利用了矩阵的结合律。在计算时，输入 $x$ 会同时流向两条路：

1. **原路**：经过 $W_0$，保持原模型的知识。
2. **旁路**：先经过 $A$ (从 $k$ 维降到 $r$ 维)，再经过 $B$ (从 $r$ 维升回 $d$ 维)。
3. **相加**：将两路的结果相加。

#### 5. 关键细节：初始化与缩放

- **初始化策略**：

  - 矩阵 $A$ 使用高斯分布随机初始化。
  - 矩阵 $B$ 初始化为 **全 0**。
  - **目的**：保证训练刚开始时，$B A = 0$，即 $\Delta W = 0$。这样模型的输出与预训练模型完全一致，确保训练是从“原版模型”平滑开始的。

- **Scaling Factor ($\alpha$)**：

  实际计算中通常会乘上一个系数 $\frac{\alpha}{r}$。这使得你在调整 $r$ 的大小时，不需要重新调整学习率。

------

### 二、 工程实践：优势与实现

#### 1. 显存占用的巨大优化

在工程上，LoRA 最大的贡献是**节省显存（VRAM）**。

- **训练时**：我们只需要存储 $A$ 和 $B$ 的梯度和优化器状态。对于一个 7B 的模型，全量微调可能需要 100GB+ 显存，而 LoRA (r=8) 可能只需要 16GB-24GB。
- **参数量**：可训练参数量通常仅为原模型的 0.01% - 0.1%。

#### 2. 推理时的“零延迟” (Zero Overhead)

这是 LoRA 优于 Adapter 或 Prefix-Tuning 的关键点。

在部署推理（Inference）时，我们可以通过**重参数化（Reparameterization）**将 LoRA 权重合并回原模型：

$$W_{new} = W_0 + B \cdot A \cdot \frac{\alpha}{r}$$

一旦合并，模型结构就变回了普通的线性层。**推理速度与原模型完全一样**，没有任何额外的计算开销。

#### 3. 多任务热插拔

因为 LoRA 权重文件非常小（通常只有几十 MB），你可以在生产环境中保留一份 base model，然后根据用户请求动态加载不同的 LoRA 适配器（比如一个用于写SQL，一个用于写诗），无需重新加载整个大模型。

#### 4. 代码实现 (基于 Python `peft` 库)

在实际工程中，我们很少徒手写矩阵乘法，通常使用 Hugging Face 的 `peft` 库。

```Python
from transformers import AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType

# 1. 加载基础模型
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-llm-7b-base")

# 2. 定义 LoRA 配置
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    inference_mode=False, 
    r=8,            # 秩，决定参数量大小
    lora_alpha=32,  # 缩放系数，通常设为 r 的 2-4 倍
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"] # 指定在哪些层应用 LoRA (通常是 Attention 的 Q, V)
)

# 3. 将模型包装为 LoRA 模型
model = get_peft_model(model, peft_config)

# 4. 打印可训练参数量
model.print_trainable_parameters()
# 输出示例: trainable params: 4,194,304 || all params: 7,000,000,000 || trainable%: 0.06
```

------

### 三、 进阶：如何调参 (Best Practices)

1. **`target_modules` 的选择**：
   - 早期主要微调 `q_proj` 和 `v_proj`。
   - **现在的共识**：**微调所有的线性层**（Q, K, V, O, 以及 FFN 中的 Gate, Up, Down）通常效果最好。虽然参数量会增加一点，但效果提升明显。
2. **`r` 和 `alpha` 的关系**：
   - 常见的做法是设 `alpha = 2 * r` 或 `alpha = r`。
   - $r$ 不一定越大越好。对于简单的指令微调，`r=8` 或 `16` 足矣；对于复杂的逻辑推理或注入新知识，可能需要 `r=64` 或 `128`。
3. **QLoRA**：
   - 如果你显存非常紧张，可以使用 QLoRA。它先将 Base Model 量化为 4-bit，然后再叠加 LoRA。这使得单张 24GB 显卡（如 3090/4090）也能微调 30B 甚至更大规模的模型。

### 总结

- **数学上**：利用矩阵分解 $W_0 + BA$，将高维更新转化为低维优化。
- **工程上**：冻结主干，仅训练旁路。
- **结果**：训练快、显存省、推理无延迟、权重文件小。


## QLoRA

**如何在一个“低精度”（模糊）的模型上，进行“高精度”的梯度计算和更新？**

简单来说，QLoRA 并不是把量化后的模型直接拿来训练，而是建立了一个**“存储-计算分离”**的机制。

### 一、 核心机制：存储是 4-bit，计算是 16-bit

你问的“从量化迁移到微调”，本质上是一个**动态的反量化（Dequantization）过程**。

#### 1. 静态存储（省显存）

在 QLoRA 中，庞大的**基座模型（Base Model）**权重被永久冻结，并以 **4-bit** 格式存储在显存中。

- 这就把显存占用压缩到了极致（比 FP16 节省 4 倍）。

#### 2. 动态计算（保精度）

当数据（Input）流经模型进行前向传播（Forward）和反向传播（Backward）时，必须进行矩阵乘法。4-bit 的整数是无法直接进行高精度微调计算的。

**QLoRA 的“迁移”动作发生在计算的瞬间：**

1. **Dequantize (解压)**：当计算需要用到某一层权重 $W$ 时，系统会瞬间把存储的 4-bit 权重**反量化**回 16-bit (BF16 或 FP16)。
2. **Compute (计算)**：使用这个临时的 16-bit 权重与输入 $X$ 进行矩阵乘法，$Y = X \cdot W_{fp16}$。
3. **Backprop (反向传播)**：梯度流过这个临时的 16-bit 权重，传递给 LoRA 的 Adapter（LoRA 的 $A$ 和 $B$ 矩阵始终保持 16-bit，从未被量化）。
4. **Discard (释放)**：计算完成后，立即释放临时的 16-bit 权重，显存中只保留 4-bit 版本。

**用一句话总结：** QLoRA 是“**用空间换时间**”的反向操作——**用微小的计算时间开销（实时反量化），换取了巨大的空间节省。**

------

### 二、 数学原理：为什么是 NF4 (NormalFloat 4-bit)？

传统的量化（Int4）通常是**线性量化**，即把数值范围均匀切分。但 QLoRA 发现这不适合神经网络。

#### 1. 权重分布的特点

预训练大模型的权重通常服从**正态分布（Normal Distribution）**，即大部分数值聚集在 0 附近，极少数数值在两端。

#### 2. 线性量化的缺陷

如果使用标准的 Int4（线性间隔）：

- 在 0 附近（数据密集区），刻度太稀疏，导致大量精度丢失。
- 在两端（数据稀疏区），刻度太密集，浪费了存储位。

#### 3. NF4 的创新：基于分位数的量化

QLoRA 发明了 **NF4 (NormalFloat 4-bit)** 数据类型。它的原理基于**分位数（Quantile）**：

- 它不是按数值距离切分，而是按**概率面积**切分。
- **核心思想**：保证每个量化桶（bin）里落入的权重数量是相等的。
- **结果**：在 0 附近分配了更多的量化刻度，在两端分配较少。

**数学表达**：

假设权重的概率密度函数为 $N(0, 1)$，NF4 的 16 个量化节点 $q_i$ 满足：

$$\int_{-\infty}^{q_i} N(x) dx = \frac{i}{16} + \frac{1}{32}$$

这使得 NF4 成为**在正态分布假设下，信息论层面最优的量化方式**。

------

### 三、 极致压缩：双重量化 (Double Quantization)

为了让显存占用降到最低，QLoRA 连“量化参数”本身也不放过。

#### 1. 量化常数 (Quantization Constants)

在量化时，我们需要记录缩放系数（Scale Factor），通常每 64 个参数（Block size = 64）共享一个 FP32 的缩放系数。

虽然每个系数很小，但参数量几十亿时，这些系数加起来也是一笔不小的显存开销（约 0.5 GB / 65B 模型）。

#### 2. 二次量化

QLoRA 对这些 FP32 的缩放系数再次进行量化（Quantize the Quantization Constants），把它们压缩成 8-bit。

- **结果**：平均每个参数仅增加 0.127 bit 的额外开销。
- 这使得 65B 的模型可以塞进一张 48GB 的显卡里训练。

------

### 四、 总结：QLoRA 的完整工作流

当你运行 QLoRA 代码时，系统内部发生了以下过程：

1. **加载阶段**：
   - 读取 Base Model 权重。
   - 使用 **NF4** 算法将其压缩，并应用 **Double Quantization**。
   - 存储在显存中（4-bit）。
2. **初始化阶段**：
   - 在 Base Model 的线性层旁边，初始化标准的 **LoRA Adapter**（$A, B$ 矩阵）。
   - LoRA 部分保持 **BF16/FP16** 精度。
3. **前向/反向传播（循环进行）**：
   - **Base Model**：4-bit -> 解压为 BF16 -> 参与矩阵乘法 -> 释放 BF16。
   - **LoRA Adapter**：全程 BF16 计算。
   - **梯度更新**：仅更新 LoRA Adapter 的参数，Base Model 保持 4-bit 冻结状态。

### 一句话总结 QLoRA vs LoRA

- **LoRA**：Base Model 是 FP16，Adapter 是 FP16。
- **QLoRA**：Base Model 是 NF4 (存储) / FP16 (计算)，Adapter 是 FP16。

这就是为什么 QLoRA 能在不损失微调效果（几乎）的前提下，让显存需求暴降的根本原因。你现在用 `bitsandbytes` 库加载模型时看到的 `load_in_4bit=True`，背后跑的就是这一套逻辑。
---

## P-Tuning v2 / Prefix Tuning
**P-Tuning v2** 和 **Prefix Tuning** 虽然名字不同，但它们在**数学本质上是高度趋同的**（可以统称为 **Deep Prompt Tuning**），它们都属于 **Soft Prompt（软提示）** 的技术路线。

如果说 LoRA 是通过“**旁路加法**”来修改模型行为，那么 P-Tuning v2 / Prefix Tuning 就是通过“**强行植入虚拟记忆**”来控制模型。

------

### 一、 数学原理：从“改参数”到“改输入”

#### 1. 核心理念：Prompt as Parameters

在传统的 Prompt Engineering 中，我们用自然语言写 prompt（例如：“将下面这句话翻译成英文：”）。这些 token 是离散的（Discrete），对应的向量是固定的。

**Soft Prompt** 的核心思想是：**为什么 Prompt 必须是人类能看懂的字？**

我们可以直接优化**连续的向量（Continuous Vectors）**。这些向量在语义空间中可能不对应任何人类语言单词，但机器能懂。

#### 2. 前身：P-Tuning v1 的局限

P-Tuning v1 仅在模型的**输入层（Embedding Layer）** 插入了一组可训练的向量 $P$。

$$Input = [P_1, P_2, \dots, P_n, X]$$

- **数学问题**：对于几十层的深层网络（Deep Network），输入层的微小扰动很难通过层层非线性变换传递到最后一层。这导致 P-Tuning v1 在复杂任务上效果不佳。

#### 3. 进阶：Prefix Tuning (2021) & P-Tuning v2 (2022)

为了解决“传递难”的问题，这两种方法引入了 **Deep Prompting（深层提示）** 机制。

**数学定义：**

假设模型有 $L$ 层 Transformer。我们在**每一层**的 Attention 计算中，都强行插入一组可训练的 **Prefix（前缀）向量**。

对于第 $l$ 层，标准的 Attention 计算是：

$$Head = \text{Attention}(Q, K, V)$$

在 Prefix Tuning / P-Tuning v2 中，我们将 Key ($K$) 和 Value ($V$) 进行拼接（Concat）：

$$K' = \text{Concat}(P_k^{(l)}, K)$$

$$V' = \text{Concat}(P_v^{(l)}, V)$$

- $P_k^{(l)}, P_v^{(l)}$：第 $l$ 层的可训练前缀参数。
- $K, V$：原始输入的 Key 和 Value（被冻结）。

**直观理解：**

这就好比在每一层 Transformer 处理信息之前，你都通过 $P_k$ 和 $P_v$ 告诉它：“**记住，之前的上下文是XXX（哪怕实际上并没有这个上下文）**”。

你相当于修改了模型的**短期记忆（Short-term Memory/Cache）**，引导它生成特定的内容。

------

### 二、 两者的细微区别（学术考古）

虽然原理几乎一样，但在工程细节上略有不同：

| **特性**       | **Prefix Tuning (Li & Liang)**                    | **P-Tuning v2 (THUDM/清华)**             |
| -------------- | ------------------------------------------------- | ---------------------------------------- |
| **主要应用**   | 生成任务 (NLG, 如摘要、翻译)                      | 全能 (NLU + NLG，强调在分类任务上也有效) |
| **参数化方式** | 使用 MLP (重参数化) 来生成 Prefix，训练完去掉 MLP | 通常直接优化向量 (去掉 MLP)，简化训练    |
| **结构**       | 每一层都加                                        | 每一层都加 (Deep Prompting)              |
| **本质**       | **一样**                                          | **一样**                                 |

*注：在现在的工程库（如 `peft`）中，这两者经常被合并或混用，因为实现逻辑是通用的。*

------

### 三、 工程实践：实现与优劣势

#### 1. 代码实现 (基于 `peft`)

在 `peft` 库中，使用 Prefix Tuning 非常简单。



```Python
from peft import get_peft_model, PrefixTuningConfig, TaskType

# 1. 配置 Prefix Tuning
peft_config = PrefixTuningConfig(
    task_type=TaskType.CAUSAL_LM, 
    num_virtual_tokens=20,  # 相当于要在前面插入多少个“虚拟单词”
    prefix_projection=False # 是否使用 MLP 重参数化 (P-Tuning v2 通常设为 False)
)

# 2. 包装模型
# model 是你加载的 HuggingFace 模型
model = get_peft_model(model, peft_config)

# 3. 观察参数
model.print_trainable_parameters()
```

#### 2. 这里的 `num_virtual_tokens` 是什么？

这是最重要的超参数。

- 如果你设为 20，意味着你在每一层都占用了 20 个 Token 的位置。
- 这些 Token 不是真实的单词，而是单纯的数学向量。

#### 3. 与 LoRA 的工程对比（关键！）

这也是为什么现在 **LoRA 比 Prefix Tuning 更火** 的原因：

- **LoRA (ResAdd)**：
  - $\Delta h$ 是直接加在隐状态上的。
  - **不占用上下文长度 (Context Length)**。
  - 推理速度**无损**（合并权重后）。
- **Prefix Tuning (Concat)**：
  - 通过拼接 $K, V$ 实现。
  - **占用上下文长度**：如果你的模型最大长度是 2048，你用了 100 个 virtual tokens，那你真正能用的就只剩 1948 了。
  - **推理变慢**：因为每次计算 Attention 时，$K, V$ 的维度变大了，计算量实打实地增加了。并且它**不能**像 LoRA 那样简单地把权重合并进去（因为是非线性的 Attention 操作）。

#### 4. 什么时候使用 P-Tuning v2 / Prefix Tuning？

尽管 LoRA 占统治地位，但 Prefix Tuning 在某些场景仍有优势：

- **个性化/多用户场景**：因为 Prefix 实际上就是一组 KV Cache。在推理时，你可以针对不同用户加载不同的 Prefix Cache，而不需要切换巨大的权重矩阵。
- **极少样本 (Few-shot)**：有些研究表明，在数据量极少的情况下，Prompt Tuning 类的收敛效果可能优于 LoRA。

### 总结

- **数学本质**：在每一层 Transformer 的 Key 和 Value 前面拼接可训练的“虚拟向量”，伪造上下文。
- **工程本质**：以**牺牲一部分上下文长度和推理速度**为代价，换取微调能力。
- **地位**：它是 LoRA 出现之前最强的 PEFT 技术，现在通常作为 LoRA 的备选方案，或者在特定的 ChatGLM 系列模型中仍然比较流行（因为 ChatGLM 团队大力推广 P-Tuning v2）。

---

