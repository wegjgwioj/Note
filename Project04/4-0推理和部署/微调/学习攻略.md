
#### 1. åŸºç¡€æ¶æ„ï¼ˆå¦‚æœä¸ç†Ÿæ‚‰ Transformerï¼Œè¯·åŠ¡å¿…å…ˆè¡¥è¯¾ï¼‰

* **Transformer:** *Attention Is All You Need*
* **BERT / GPT-3:** ç†è§£ Encoder-only, Decoder-only æ¶æ„çš„åŒºåˆ«ã€‚

#### 2. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰â€”â€”

è¿™æ˜¯å¾®è°ƒå²—ä½çš„æ ¸å¿ƒã€‚ä½ éœ€è¦ä¸ä»…çŸ¥é“æ€ä¹ˆç”¨ï¼Œè¿˜è¦çŸ¥é“**ä¸ºä»€ä¹ˆ**è¿™æ ·èƒ½çœæ˜¾å­˜ã€‚

* **LoRA (Low-Rank Adaptation):** *LoRA: Low-Rank Adaptation of Large Language Models*
* **æ ¸å¿ƒè€ƒç‚¹ï¼š** ä¸ºä»€ä¹ˆä½ç§©çŸ©é˜µèƒ½æ¨¡æ‹Ÿå…¨é‡å¾®è°ƒï¼Ÿ å’Œ  çŸ©é˜µçš„åˆå§‹åŒ–æœ‰ä»€ä¹ˆè®²ç©¶ï¼Ÿ


* **Q-LoRA:** *QLoRA: Efficient Finetuning of Quantized LLMs*
* **æ ¸å¿ƒè€ƒç‚¹ï¼š** 4-bit é‡åŒ–å¯¹ç²¾åº¦çš„å½±å“ï¼ŒNF4 æ•°æ®ç±»å‹æ˜¯ä»€ä¹ˆã€‚


* **Prefix-Tuning / P-Tuning v2:** äº†è§£ Soft Prompt çš„æ¦‚å¿µã€‚

#### 3. å¯¹é½ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆè¿›é˜¶åŠ åˆ†é¡¹ï¼‰

* **RLHF:** *Training language models to follow instructions with human feedback* (InstructGPT)
* **DPO:** *Direct Preference Optimization* (ç›®å‰æ¯” PPO æ›´æµè¡Œï¼Œæ•°å­¦åŸç†æ›´ç®€æ´)ã€‚

---

### ğŸ› ï¸ ç¬¬äºŒé˜¶æ®µï¼šå·¥å…·é“¾ä¸å®æ“é¡¹ç›®

å…‰çœ‹ä¸ç»ƒæ˜¯æ— æ³•é€šè¿‡ä»£ç é¢çš„ã€‚ä½ éœ€è¦ç†Ÿæ‚‰ä»¥ä¸‹â€œå†›ç«åº“â€ï¼š

#### å¿…å¤‡å·¥å…·é“¾

* **PyTorch:** æ·±åº¦å­¦ä¹ åŸºåº§ã€‚
* **Hugging Face Transformers / PEFT / Datasets:** å·¥ä¸šç•Œæ ‡å‡†åº“ã€‚
* **DeepSpeed / Megatron-LM:** åˆ†å¸ƒå¼è®­ç»ƒç¥å™¨ï¼ˆå¿…å­¦ Zero-2, Zero-3 Offload ç­–ç•¥ï¼‰ã€‚
* **Bitsandbytes:** é‡åŒ–åº“ã€‚







