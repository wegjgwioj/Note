# “计算层 -> 显存层 -> 模型层 -> 策略层 -> 调度层”
---
# 推理领域的全景图

### 1. 策略层 (Decoding Strategy) — “怎么生成？”

这一层决定了 Token 是怎么选出来的，也是**投机采样**的老家。

* **常规采样 (Standard Sampling):**
* `Greedy Search`: 贪婪搜索，只选概率最高的。
* `Beam Search`: 束搜索，保留 K 条路径（现在在大模型中用得少，太慢）。
* `Top-k / Top-p (Nucleus)`: 核采样，增加多样性。


* **加速采样 (Speculative Decoding Family):**  
* `Draft-Verification`: 经典的大小模型（Llama-7B + TinyLlama）。
* `Medusa / Eagle`: **非自回归头**。在模型上加几个头，一次预测多个 Token。
* `Lookahead / Prompt Lookup`: **N-gram 匹配**。利用上下文重复性，不需要小模型，直接猜后面会重复前面的词。
* `Speculative Sampling`: 也就是你提到的投机采样，特指基于拒绝采样（Rejection Sampling）的概率修正算法。


### 2. 模型压缩层 (Model Optimization) — “怎么变小？”

这一层为了降低显存占用和带宽压力，甚至不惜牺牲一点点精度。

* **权重量化 (Weight Quantization):**
* `INT8 / INT4`: 仅压缩权重，推理时解压。
* `GPTQ / AWQ`: 激活感知的量化，保证重要权重的精度。
* `GGUF (k-quants)`: 也就是 llama.cpp 的核心，针对 CPU/Apple Silicon 优化的量化格式。

* **KV Cache 量化:**
* `KV Cache INT8 / FP8`: 压缩历史对话的缓存，这对于**长文本（Long Context）**场景是救命稻草。

* **稀疏化 (Sparsity):**
* `SparseGPT` / `Wanda`: 剪枝，把不重要的权重变成 0，不再计算。


### 3. 显存管理层 (Memory Management) — “怎么存放？”

这一层是推理系统的核心瓶颈（Memory Bound）所在，也是 vLLM 称霸的原因。

* **分页管理:**
* `PagedAttention`: 类似操作系统的虚拟内存，解决 KV Cache 的碎片化问题。

* **共享前缀:**
* `RadixAttention`: 多个用户问同样的问题（如 System Prompt 一样），显存里只存一份，大家共享。

* **注意力结构优化:**
* `MQA (Multi-Query Attention)`: 强行让所有的 Head 共享同一组 KV，显存占用直接除以 Head 数。
* `GQA (Grouped-Query Attention)`: Llama-3/DeepSeek 的标配，折中方案，几个 Head 共享一组 KV。



### 4. 算子/计算层 (Kernel Optimization) — “怎么计算？”


* **注意力加速:**
* `FlashAttention (v1/v2/v3)`: 通过 Tiling（分块）和 Recomputation，减少 HBM（显存）读写，IO 效率提升数倍。
* `FlashDecoding`: 针对推理阶段（Decode）特别优化的 FlashAttention 版本。


* **算子融合 (Kernel Fusion):**
* 把 `LayerNorm` + `Activation` + `MatMul` 合并成一个 CUDA Kernel，减少启动开销。

### 5. 系统调度层 (Serving System) — “怎么服务？”

这一层决定了怎么同时伺候 1000 个用户。

* **批处理策略:**
* `Continuous Batching (Orca)`: 连续批处理。不等待一个 Batch 全部跑完，先跑完的先走，新请求随时插队进来。


* **架构分离:**
* `Prefill-Decode Separation (Disaggregated Serving)`: **PD 分离**。
* 一群机器专门负责 Prefill（首字生成，算力密集）。
* 一群机器专门负责 Decode（后续生成，带宽密集）。
* 中间通过 KV Cache 传输数据。


---
